# Generative AI With LLM's

Welcome to the "Generative AI With LLM's" repository! In this project, I explore the fascinating world of Generative Artificial Intelligence, focusing on Large Language Models (LLM's). The primary objective is to understand and implement generative capabilities using transformer architectures, with a special emphasis on the Flan-T5 model from Hugging Face.

## Table of Contents
- [Introduction](#introduction)
- [Getting Started](#getting-started)
- [Model Architecture](#model-architecture)
- [Zero, Few, and One Shot Learning](#zero-few-and-one-shot-learning)
- [Full-Fine Tuning](#full-fine-tuning)
- [Evaluation](#evaluation)
- [PEFT and LoRA](#peft-and-lora)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## Introduction
In the initial stages of this project, I delved into the fundamentals of Generative AI, with a specific focus on understanding the transformer architecture. Subsequently, I implemented the Flan-T5 model using the Hugging Face library, which serves as the backbone for the generative capabilities explored in this repository.

## Getting Started
To get started with the project, follow these steps:
1. Clone the repository: `git clone https://github.com/realfaiq/Generative-AI-With-LLMS`
2. Install dependencies.
3. Explore the Jupyter notebooks to understand the code implementations.

## Model Architecture
The core of this project is the utilization of the Flan-T5 model, a powerful transformer architecture known for its language generation capabilities. The model's architecture is discussed and implemented in detail within the codebase.

## Zero, Few, and One Shot Learning
To enhance the performance of the Flan-T5 model, I implemented strategies such as zero-shot, few-shot, and one-shot learning. These techniques aim to improve the model's ability to generate coherent and contextually relevant text based on varying levels of input information.

## Full-Fine Tuning
In the pursuit of achieving optimal performance, I conducted full-fine tuning of the model. This involved training the Flan-T5 on specific datasets relevant to the project's objectives.

## Evaluation
To assess the quality of the generative outputs, I employed the ROUGE score as a metric for evaluation. This allows for a quantitative measure of the model's performance in terms of text similarity and coherence.

## PEFT and LoRA
A significant aspect of this project involves the use of Progressive Embedding Fine-Tuning (PEFT) and, more specifically, Low Resource Adaptation (LoRA). These techniques were employed to further fine-tune the model and enhance its generative capabilities.

## Results
Details of the results obtained from the various stages of model training, fine-tuning, and evaluation are documented in the project's notebooks and can be explored to gain insights into the model's performance.

## Contributing
Contributions to this project are welcome! Feel free to fork the repository, create branches, and submit pull requests. If you have ideas for improvement or new features, open an issue to discuss them.

## License
This project is licensed under the MIT License. Feel free to use, modify, and distribute the code as per the terms of the license.

Happy Generative AI coding! ðŸ¤–ðŸ“š
